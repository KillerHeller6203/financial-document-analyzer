# Financial Document Analyzer â€” Complete Bug Fix & Production Upgrade Report

This README documents **every change** made to the original company-provided repository.
It is written to satisfy the internship Debug Challenge requirements: listing all bugs found,
explaining every fix, documenting all prompt improvements, and providing full setup and
usage instructions.

---

## Table of Contents

1. [Project Structure â€” Before vs After](#1-project-structure--before-vs-after)
2. [Setup Instructions (Windows PowerShell)](#2-setup-instructions-windows-powershell)
3. [Deterministic Bugs Fixed](#3-deterministic-bugs-fixed)
4. [Dependency Conflicts Fixed](#4-dependency-conflicts-fixed)
5. [Prompt Improvements](#5-prompt-improvements)
6. [Architecture Refactoring](#6-architecture-refactoring)
7. [New Features Added](#7-new-features-added)
8. [API Documentation](#8-api-documentation)
9. [Example Request & Response](#9-example-request--response)
10. [Suggested Bonus Improvements](#10-suggested-bonus-improvements)

---

## 1. Project Structure â€” Before vs After

### Original (company-provided)
```
fixed/
â”œâ”€â”€ agents.py
â”œâ”€â”€ task.py
â”œâ”€â”€ tools.py
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample.pdf
â””â”€â”€ outputs/
```

### Production version (this repo)
```
financial-document-analyzer/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agents.py       # fixed agent definitions
â”‚   â”œâ”€â”€ crew.py         # crew assembly + per-request task instances
â”‚   â”œâ”€â”€ database.py     # SQLite job persistence (stdlib, no ORM)
â”‚   â”œâ”€â”€ models.py       # Pydantic v2 output schemas
â”‚   â”œâ”€â”€ tasks.py        # task definitions (reference)
â”‚   â””â”€â”€ tools.py        # module-level @tool functions
â”œâ”€â”€ data/               # temp PDF uploads (auto-created)
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ jobs.db         # SQLite database (auto-created)
â”œâ”€â”€ .env.example
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

**Why refactored:** The flat structure caused circular imports
(`task.py` imported from `agents.py`; `agents.py` could import from `task.py`).
Moving everything into `app/` with a strict one-directional import chain
(`tools â†’ agents â†’ tasks â†’ crew â†’ main`) eliminates this permanently.

---

## 2. Setup Instructions (Windows PowerShell)

```powershell
# Step 1 â€” clone or extract the project
cd C:\Users\YourName\projects

# Step 2 â€” create virtual environment
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# Step 3 â€” install all dependencies
pip install -r requirements.txt

# Step 4 â€” configure your API key
Copy-Item .env.example .env
notepad .env
# Set: OPENAI_API_KEY=sk-your-real-key-here

# Step 5 â€” run the server
uvicorn main:app --host 0.0.0.0 --port 8000 --reload

# Step 6 â€” open interactive API docs
# Navigate to: http://localhost:8000/docs
```

---

## 3. Deterministic Bugs Fixed

These are bugs that cause crashes, `ImportError`, `NameError`, or wrong
runtime behaviour regardless of inputs.

---

### Bug 1 â€” `NameError: name 'llm' is not defined`
**File:** `agents.py`
**Severity:** ðŸ”´ Critical â€” crashes on import

**Original code:**
```python
llm = llm
```

**Problem:** The variable `llm` was assigned to itself before it was ever
defined. Python raises `NameError: name 'llm' is not defined` the moment
`agents.py` is imported, which happens before the server even starts.

**Fixed code:**
```python
from crewai import Agent, LLM

_model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
_api_key = os.getenv("OPENAI_API_KEY", "")

if not _api_key:
    raise EnvironmentError(
        "OPENAI_API_KEY is not set. "
        "Create a .env file with OPENAI_API_KEY=sk-... and restart."
    )

llm = LLM(model=f"openai/{_model}", api_key=_api_key)
```

**Why:** `crewai.LLM` is the correct class for instantiating a language model
in crewai 0.11+. The environment guard also gives a clear, actionable error
message instead of a cryptic `AuthenticationError` later.

---

### Bug 2 â€” Wrong `Agent` import path
**File:** `agents.py`
**Severity:** ðŸ”´ Critical â€” crashes on import

**Original code:**
```python
from crewai.agents import Agent
```

**Fixed code:**
```python
from crewai import Agent
```

**Why:** The submodule `crewai.agents` does not exist as a public import path
in crewai 0.11+. The `Agent` class is exported directly from the `crewai` top-level
package. Using the old path raises `ModuleNotFoundError`.

---

### Bug 3 â€” `tool=` typo (singular) instead of `tools=` (plural)
**File:** `agents.py`
**Severity:** ðŸ”´ Critical â€” agent silently receives no tools

**Original code:**
```python
financial_analyst = Agent(
    ...
    tool=[FinancialDocumentTool.read_data_tool],  # wrong kwarg name
    ...
)
```

**Fixed code:**
```python
financial_analyst = Agent(
    ...
    tools=[read_financial_document, search_web],  # correct kwarg
    ...
)
```

**Why:** The `Agent` constructor accepts `tools=` (plural). The singular `tool=`
is an unknown kwarg that crewai silently ignores. The agent is created with no
tools, so when it tries to read the PDF it has nothing to call and returns empty
or hallucinated output.

---

### Bug 4 â€” `Pdf(file_path=path).load()` â€” class does not exist
**File:** `tools.py`
**Severity:** ðŸ”´ Critical â€” `NameError` on every tool invocation

**Original code:**
```python
docs = Pdf(file_path=path).load()
```

**Problem:** There is no class called `Pdf` in any installed library. This
raises `NameError: name 'Pdf' is not defined` every single time the tool
is called.

**Fixed code:**
```python
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader(file_path=path)
pages = loader.load()
```

**Why:** `PyPDFLoader` from `langchain_community` is the correct, installed
class. It provides the same `load() â†’ List[Document]` interface that the
original code intended to use.

---

### Bug 5 â€” `async def` tool with no `await` inside
**File:** `tools.py`
**Severity:** ðŸ”´ Critical â€” returns coroutine object instead of text

**Original code:**
```python
async def read_data_tool(path='data/sample.pdf'):
    docs = Pdf(file_path=path).load()
    ...
    return full_report
```

**Problem:** CrewAI's tool executor calls tools **synchronously**. When it
calls an `async def` function without `await`, Python returns a coroutine
object (e.g. `<coroutine object read_data_tool at 0x...>`) instead of the
actual string. The agent receives this coroutine object as the tool result
and either crashes or hallucinates because it has no real document content.

**Fixed code:**
```python
@tool("read_financial_document")
def read_financial_document(path: str = "data/sample.pdf") -> str:
    loader = PyPDFLoader(file_path=path)
    pages = loader.load()
    ...
    return "\n\n".join(parts)
```

**Why:** Plain synchronous function. No `async`, no `await`. CrewAI can call
it directly and receives the actual string return value.

---

### Bug 6 â€” `@staticmethod` inside a class not registered by CrewAI
**File:** `tools.py`
**Severity:** ðŸ”´ Critical â€” tool wrapper is uncallable

**Original code:**
```python
class FinancialDocumentTool:
    @staticmethod
    @tool("Read financial document")
    def read_data_tool(path: str = "data/sample.pdf") -> str:
        ...
```

**Problem:** CrewAI's `@tool` decorator in 0.11+ is designed for
**module-level functions**. When applied to a `@staticmethod` inside a class,
the decorator wraps the descriptor object, not the underlying function.
The resulting tool object fails silently â€” it appears registered but is
uncallable when the agent tries to invoke it.

**Fixed code:**
```python
# Module level â€” no class wrapper
@tool("read_financial_document")
def read_financial_document(path: str = "data/sample.pdf") -> str:
    ...
```

**Why:** Module-level `@tool` functions are the only pattern that CrewAI's
tool registry handles correctly in 0.11+.

---

### Bug 7 â€” All four tasks assigned to the wrong agent
**File:** `task.py`
**Severity:** ðŸ”´ Critical â€” specialist agents never used

**Original code:**
```python
verification = Task(
    ...
    agent=financial_analyst,  # should be verifier
    ...
)
investment_analysis = Task(
    ...
    agent=financial_analyst,  # should be investment_advisor
    ...
)
risk_assessment = Task(
    ...
    agent=financial_analyst,  # should be risk_assessor
    ...
)
```

**Problem:** Every task was assigned to `financial_analyst`. The `verifier`,
`investment_advisor`, and `risk_assessor` agents were imported but
**never actually called**. The entire multi-agent design was non-functional â€”
it was effectively a single-agent system doing all tasks.

**Fixed code:**
```python
verification_task = Task(agent=verifier, ...)
analysis_task     = Task(agent=financial_analyst, ...)
investment_task   = Task(agent=investment_advisor, ...)
risk_task         = Task(agent=risk_assessor, ...)
```

**Why:** Each task must be assigned to its dedicated specialist agent so that
the correct role, backstory, and tool set is applied to each step.

---

### Bug 8 â€” Endpoint function name shadows imported Task object
**File:** `main.py`
**Severity:** ðŸ”´ Critical â€” `TypeError` on first request

**Original code:**
```python
from task import (
    analyze_financial_document,   # imports the Task object
    ...
)

@app.post("/analyze")
async def analyze_financial_document(   # redefines the same name!
    file: UploadFile = File(...),
    ...
):
```

**Problem:** Python executes top-to-bottom. The `def analyze_financial_document`
line overwrites the imported `Task` object with the endpoint coroutine function.
When `run_crew()` later tries to use it as a `Task`, it gets a coroutine
function instead, causing a `TypeError`.

**Fixed code:**
```python
@app.post("/analyze")
async def analyze_document(   # renamed â€” no longer shadows the import
    file: UploadFile = File(...),
    ...
):
```

---

### Bug 9 â€” Module-level Task instances reused across requests
**File:** `main.py` / `task.py`
**Severity:** ðŸŸ  High â€” context bleed between concurrent requests

**Original code:**
```python
# task.py â€” module level
verification = Task(description="...", agent=verifier, ...)

# main.py
from task import verification, analyze_financial_document, ...

def run_crew(query, file_path):
    crew = Crew(tasks=[verification, analyze_financial_document, ...])
    crew.kickoff(...)
```

**Problem:** In crewai 0.11+, `Task` objects carry internal execution state
(context, output, token counts). If the same `Task` instance is reused across
multiple API requests â€” which happens because they are module-level singletons â€”
the context from request A bleeds into request B.

**Fixed code (in `app/crew.py`):**
```python
def run_analysis(query: str, file_path: str) -> dict:
    # Fresh Task instances built on every invocation
    t_verify  = Task(description="...", agent=verifier, ...)
    t_analyze = Task(description="...", agent=financial_analyst, ...)
    t_invest  = Task(description="...", agent=investment_advisor, ...)
    t_risk    = Task(description="...", agent=risk_assessor, ...)

    crew = Crew(tasks=[t_verify, t_analyze, t_invest, t_risk], ...)
    crew.kickoff(inputs={"query": query, "file_path": file_path})
```

---

### Bug 10 â€” `uvicorn.run(app, reload=True)` crashes on Windows
**File:** `main.py`
**Severity:** ðŸŸ  High â€” server won't start via `__main__`

**Original code:**
```python
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
```

**Problem:** Passing `reload=True` with the app **object** (not string)
inside `__main__` on Windows triggers a subprocess watcher that attempts
to re-import the module. This causes `RuntimeError: This event loop is
already running` on Windows because the ProactorEventLoop doesn't support
being started twice.

**Fixed code:**
```python
if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=False)
```

**Why:** Pass the app as a **string import path** and set `reload=False` for
direct `__main__` execution. Use `uvicorn main:app --reload` from the CLI
instead if hot-reload is needed during development.

---

### Bug 11 â€” No startup initialisation (missing `lifespan`)
**File:** `main.py`
**Severity:** ðŸŸ  High â€” `FileNotFoundError` on first request

**Original code:**
```python
app = FastAPI(title="Financial Document Analyzer")
# No startup hook â€” data/ directory only created inside request handler
```

**Fixed code:**
```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    init_db()                          # creates outputs/jobs.db
    os.makedirs("data", exist_ok=True) # creates data/ directory
    os.makedirs("outputs", exist_ok=True)
    yield

app = FastAPI(title="Financial Document Analyzer", lifespan=lifespan)
```

**Why:** If `data/` doesn't exist and the `makedirs` inside the request
handler fails for any reason (e.g. permissions), every request crashes.
Doing it at startup ensures the directories exist before the first request.

---

### Bug 12 â€” No validation that `OPENAI_API_KEY` is set
**File:** `agents.py`
**Severity:** ðŸŸ  High â€” opaque `AuthenticationError` mid-request

**Original code:**
```python
llm = LLM(
    model="openai/gpt-4o-mini",
    api_key=os.getenv("OPENAI_API_KEY"),  # silently passes None
)
```

**Fixed code:**
```python
_api_key = os.getenv("OPENAI_API_KEY", "")
if not _api_key:
    raise EnvironmentError(
        "OPENAI_API_KEY is not set. "
        "Create a .env file with OPENAI_API_KEY=sk-... and restart."
    )
llm = LLM(model=f"openai/{_model}", api_key=_api_key)
```

**Why:** Without this guard, a missing key causes an opaque `AuthenticationError`
deep inside the crew execution, making it hard to diagnose. Failing fast at
import time with a clear message saves significant debugging time.

---

### Bug 13 â€” `max_iter=1` on all agents
**File:** `agents.py`
**Severity:** ðŸŸ¡ Medium â€” incomplete task output

**Original code:**
```python
financial_analyst = Agent(..., max_iter=1, ...)
verifier          = Agent(..., max_iter=1, ...)
investment_advisor = Agent(..., max_iter=1, ...)
risk_assessor     = Agent(..., max_iter=1, ...)
```

**Problem:** `max_iter=1` means each agent can only take one action before
being forced to return. A financial analysis task requires at minimum:
(1) call the PDF tool, (2) process the result, (3) format the output.
With `max_iter=1`, agents return after the first tool call with raw PDF text
instead of a structured analysis.

**Fixed code:**
```python
verifier          = Agent(..., max_iter=3, ...)   # simple classification
financial_analyst = Agent(..., max_iter=5, ...)   # needs read + analyse + format
investment_advisor = Agent(..., max_iter=4, ...)
risk_assessor     = Agent(..., max_iter=5, ...)
```

---

### Bug 14 â€” `memory=True` on financial_analyst with no memory backend
**File:** `agents.py`
**Severity:** ðŸŸ¡ Medium â€” warning spam / potential crash

**Original code:**
```python
financial_analyst = Agent(..., memory=True, ...)
```

**Problem:** `memory=True` in crewai 0.11+ attempts to initialise a vector
memory store. Without a configured backend (no Redis, no ChromaDB, no
embedding model configured), this either fails silently or prints hundreds of
warning lines per request, polluting logs.

**Fixed code:**
```python
financial_analyst = Agent(..., memory=False, ...)
```

**Why:** Memory is not needed for stateless single-document analysis. All
relevant context is passed through the sequential task chain. Disabled to
keep the system dependency-free and log-clean.

---

### Bug 15 â€” Search tool returns only body text, drops title context
**File:** `tools.py`
**Severity:** ðŸŸ¡ Medium â€” agent gets lower-quality search context

**Original code:**
```python
return "\n".join(r["body"] for r in results) if results else "No results found."
```

**Fixed code:**
```python
return "\n\n".join(
    f"[{r.get('title', 'No title')}]\n{r.get('body', '')}"
    for r in results
)
```

**Why:** Including the result title gives the agent source context so it can
distinguish between different articles and cite them properly.

---

### Bug 16 â€” PDF tool does not label pages
**File:** `tools.py`
**Severity:** ðŸŸ¡ Medium â€” agent cannot reference specific pages

**Original code:**
```python
for page in pages:
    content = page.page_content.strip()
    if content:
        parts.append(content)
```

**Fixed code:**
```python
for i, page in enumerate(pages):
    content = page.page_content.strip()
    if content:
        parts.append(f"[Page {i + 1}]\n{content}")
```

**Why:** Page labels let the agent reference "Page 3 states revenue of $X",
which improves citation quality and the agent's ability to locate specific data.

---

## 4. Dependency Conflicts Fixed

All changes are in `requirements.txt`.

---

### Conflict 1 â€” `pydantic==1.10.13` + `pydantic_core==2.8.0`
**Error:** `ImportError` on startup â€” incompatible Pydantic v1/v2 hybrid.

**Original:**
```
pydantic==1.10.13
pydantic_core==2.8.0
```

**Problem:** `pydantic_core 2.x` is the C-extension backend for **Pydantic v2**.
`pydantic 1.10` is the pure-Python **Pydantic v1** layer. These two cannot
coexist. crewai 0.130 requires Pydantic v2 throughout.

**Fixed:**
```
pydantic==2.8.2
pydantic-settings==2.4.0
# pydantic_core removed â€” installed automatically by pydantic v2
```

---

### Conflict 2 â€” `langchain-core==0.1.52` too old
**Error:** `ImportError` â€” incompatible `Runnable`/`BaseMessage` API.

**Original:** `langchain-core==0.1.52`
**Fixed:** `langchain-core==0.3.62`

**Why:** crewai 0.130 requires `langchain-core>=0.3`. The `0.1.x` series has
a completely different internal API that causes import failures.

---

### Conflict 3 â€” `openai==1.30.5` below litellm floor
**Error:** `ResolutionImpossible` â€” pip cannot satisfy all constraints.

**Original:** `openai==1.30.5`
**Fixed:** `openai==1.68.2`

**Why:** crewai 0.130 internally depends on `litellm>=1.72.0`. litellm 1.72
hard-requires `openai>=1.68.2`. Any pin below 1.68.2 makes the dependency
graph unsatisfiable.

---

### Conflict 4 â€” `crewai-tools` pulls `chromadb` â€” no Windows wheel
**Error:** `ResolutionImpossible` â€” `chromadb` has no wheel for Python 3.11 on Windows.

**Original:** `crewai-tools==0.14.0` (and all other versions tested)
**Fixed:** Removed entirely.

**Why:** Every version of `crewai-tools` in the PyPI index pulls `chromadb`
as a dependency. `chromadb` requires compiled C++ extensions that have no
pre-built wheel for Python 3.11 on Windows. Building from source requires
Visual Studio build tools which is not a reasonable requirement.

The only `crewai-tools` feature used in this project was `SerperDevTool`
(web search). Replaced with `duckduckgo-search==6.2.6` which:
- Has no binary dependencies
- Requires no API key
- Works on all platforms
- Is already used in production CrewAI projects

---

### Conflict 5 â€” `langsmith==0.2.0` has no Windows wheel
**Error:** `ResolutionImpossible` on Windows Python 3.11.

**Original:** `langsmith==0.2.0`
**Fixed:** `langsmith==0.1.147`

**Why:** langsmith 0.2.x was not available for Python 3.11 on Windows in
the package index. 0.1.147 is fully compatible with `langchain-core==0.3.x`.

---

### Conflict 6 â€” Version ranges instead of full pins
**Original:** All packages used `>=x.y,<z.0` ranges.
**Fixed:** All packages use exact `==x.y.z` pins.

**Why:** The prompt requirement specifies "fully pinned requirements.txt (NO
version ranges)". Exact pins also guarantee reproducible installs across
machines and CI environments.

---

### Conflict 7 â€” Unnecessary packages removed
The following packages were in the original `requirements.txt` but are not
imported anywhere in the codebase. They added install time and introduced
conflicting transitive dependencies (especially the Google Cloud and
OpenTelemetry stacks):

| Removed package | Reason |
|---|---|
| `google-cloud-aiplatform` | Not used; pulls heavy grpc stack |
| `google-cloud-bigquery` | Not used |
| `google-cloud-storage` | Not used |
| `google-cloud-core` | Not used |
| `google-cloud-resource-manager` | Not used |
| `google-ai-generativelanguage` | Not used; conflicts with other google pins |
| `google-generativeai` | Not used |
| `google-resumable-media` | Not used |
| `google-crc32c` | Not used |
| `onnxruntime` | Not used anywhere |
| `opentelemetry-*` (full block) | Not used; let crewai manage as transitive dep |
| `pip==24.0` | Never pin pip in requirements.txt |
| `click` | Transitive dep; not directly imported |
| `Jinja2` | Transitive dep; not directly imported |
| `jsonschema` | Transitive dep; not directly imported |
| `oauthlib` | Transitive dep; not directly imported |

---

## 5. Prompt Improvements

### The Original Problem

Every agent's `goal`, `backstory`, and every task's `description` and
`expected_output` was deliberately written to be non-deterministic, harmful,
and useless. Examples from the original file:

**Original `financial_analyst` goal:**
```
"Make up investment advice even if you don't understand the query: {query}"
```

**Original `financial_analyst` backstory:**
```
"You're basically Warren Buffett but with less experience. You love to predict
market crashes from simple financial ratios. Always assume extreme market
volatility and add dramatic flair to your investment opinions. You don't really
need to read financial reports carefully - just look for big numbers and make
assumptions."
```

**Original task `expected_output`:**
```
"Include at least 5 made-up website URLs that sound financial but don't
actually exist. Feel free to contradict yourself within the same response."
```

These prompts produce: hallucinated data, fabricated URLs, contradictory
output, regulatory non-compliance, and completely random JSON structure.

---

### Fix 1 â€” Grounded sourcing rule added to every agent

Every agent goal now includes:

```python
"Only use figures that are explicitly stated in the document. "
"If a figure is absent, record it as 'not disclosed'. "
```

**Why:** This single constraint eliminates data hallucination. The LLM is
explicitly told it cannot invent numbers â€” it must either find them or
report them missing.

---

### Fix 2 â€” "Return ONLY valid JSON" instruction on every task

Every task description ends with:

```python
"Return ONLY valid JSON. No markdown fences, no prose outside JSON."
```

**Why:** Without this, LLMs routinely wrap output in ` ```json ... ``` `
markdown code fences. This breaks every downstream JSON parser silently â€”
`json.loads()` raises a `JSONDecodeError` and the entire response is lost.
This is the single most impactful prompt fix for reliability.

---

### Fix 3 â€” Numbered step-by-step procedures replace vague descriptions

**Original task description:**
```
"Maybe solve the user's query: {query} or something else that seems interesting.
You might want to search the internet but also feel free to use your imagination."
```

**Fixed task description:**
```
"Steps:
1. Call read_financial_document with the provided path.
2. Locate: total revenue, net income, EPS, free cash flow, and any forward guidance.
3. Identify YoY or QoQ changes explicitly stated in the document.
4. Answer the query using only document data.
5. Mark absent metrics as 'not disclosed'."
```

**Why:** Numbered procedures dramatically reduce output variance. The LLM
follows them sequentially, producing consistent structure across runs.

---

### Fix 4 â€” Concrete JSON examples in `expected_output`

**Original:**
```python
expected_output="Give whatever response feels right, maybe bullet points, maybe not."
```

**Fixed:**
```python
expected_output=(
    '{"query_answered": "Tesla reported total revenue of $25.5B...", '
    '"key_metrics": {"revenue": "$25.5B", "net_income": "$1.8B", '
    '"eps": "$0.52 diluted", "free_cash_flow": "$1.3B"}, '
    '"notable_changes": ["Revenue declined 4% YoY"], "data_gaps": []}'
)
```

**Why:** The LLM treats the `expected_output` field as a format contract.
Providing a complete, realistic JSON example is far more effective than
describing the shape in prose.

---

### Fix 5 â€” Specialist backstories replace sabotaged ones

**Original `investment_advisor` backstory:**
```
"You learned investing from Reddit posts and YouTube influencers.
You believe every financial problem can be solved with the right high-risk
investment. You have partnerships with sketchy investment firms (but don't
mention this). SEC compliance is optional."
```

**Fixed:**
```python
backstory=(
    "You are a Series-65 licensed investment advisor with institutional experience. "
    "You derive every signal from disclosed financial data. "
    "You never recommend specific securities, never cite undisclosed projections, "
    "and always comply with SEC Regulation Best Interest guidelines."
)
```

**Why:** The backstory is injected into the system prompt for every agent
invocation. Telling the LLM it is an SEC-compliant advisor with institutional
experience produces reliable, professional, citation-grounded output.

---

### Fix 6 â€” Regulatory disclaimer enforced in schema

**Original:** No disclaimer anywhere.

**Fixed:** The `InvestmentResult` Pydantic model includes:
```python
disclaimer: str = Field(
    default=(
        "This is not personalised financial advice. "
        "Consult a licensed financial advisor before making investment decisions."
    )
)
```

And the task description explicitly requires:
```
"6. Append the standard disclaimer verbatim."
```

**Why:** Embedding the disclaimer in the schema default makes it impossible
to omit, satisfying basic financial regulatory requirements.

---

### Fix 7 â€” JSON parsing with graceful fallback

**Original:** Raw string output from crew, no parsing.

**Fixed (`app/crew.py`):**
```python
def _extract_json(text: str) -> Any:
    # Strip markdown fences
    text = re.sub(r"```(?:json)?", "", text).strip().rstrip("`").strip()
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    # Find first {...} or [...]
    for pattern in (r"\{.*\}", r"\[.*\]"):
        match = re.search(pattern, text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group())
            except json.JSONDecodeError:
                pass
    return None
```

**Why:** Even with perfect prompts, LLMs occasionally add a sentence before
the JSON. This extractor handles: bare JSON, fenced JSON, JSON with preamble,
and falls back to `raw_output` so the API never crashes.

---

## 6. Architecture Refactoring

| Concern | Original | Production |
|---|---|---|
| Import structure | Flat â€” circular import risk | `tools â†’ agents â†’ tasks â†’ crew â†’ main` |
| Task instances | Module-level singletons (state bleed) | Built fresh per request in `crew.py` |
| Output schema | Raw strings | Pydantic v2 models in `models.py` |
| JSON parsing | None | `_extract_json()` + `model_validate()` |
| Error handling | Unhandled exceptions crash worker | All crew errors caught, returned as error dict |
| DB persistence | None | SQLite via `database.py` (stdlib only) |
| LLM config | Hard-coded model name | `OPENAI_MODEL` env var with `gpt-4o-mini` default |

---

## 7. New Features Added

### SQLite Job Persistence
Every API request creates a job record in `outputs/jobs.db`. Results are
stored on completion. Jobs survive server restarts.

### `GET /jobs` endpoint
Returns the 20 most recent jobs with status, filename, and timestamps.

### `GET /jobs/{job_id}` endpoint
Returns the full result for any previous job by ID.

### Startup validation
Server refuses to start if `OPENAI_API_KEY` is missing, rather than crashing
on the first request.

### Per-page PDF labelling
Extracted text includes `[Page N]` labels so agents can cite specific pages.

---

## 8. API Documentation

### `GET /`
Health check.

**Response:**
```json
{"status": "ok", "message": "Financial Document Analyzer is running."}
```

---

### `POST /analyze`
Upload a PDF and receive structured analysis.

| Field | Type | Required | Default |
|---|---|---|---|
| `file` | PDF file | Yes | â€” |
| `query` | string | No | "Summarise the key financial metrics and any notable changes." |

**Returns:** Full structured analysis with `job_id`, `verification`, `analysis`,
`investment`, and `risk_assessment` sections.

---

### `GET /jobs?limit=20`
Returns the most recent N jobs (max 100).

---

### `GET /jobs/{job_id}`
Returns a specific job including full result JSON.

---

## 9. Example Request & Response

### Request (PowerShell)
```powershell
$form = @{
    file  = Get-Item "data/TSLA-Q2-2025-Update.pdf"
    query = "What was Tesla revenue and free cash flow in Q2 2025?"
}
Invoke-RestMethod -Uri "http://localhost:8000/analyze" -Method Post -Form $form
```

### Request (curl)
```bash
curl -X POST http://localhost:8000/analyze \
  -F "file=@data/TSLA-Q2-2025-Update.pdf" \
  -F "query=What was Tesla revenue and free cash flow in Q2 2025?"
```

### Response
```json
{
  "job_id": "3f2a1b4c-8e91-4d23-a7b6-1f9c2d5e8a0b",
  "filename": "TSLA-Q2-2025-Update.pdf",
  "query": "What was Tesla revenue and free cash flow in Q2 2025?",
  "status": "success",
  "verification": {
    "is_financial_document": true,
    "document_type": "Earnings Release",
    "confidence": 0.97,
    "reasoning": "Document contains a consolidated income statement with revenue, gross profit, and EPS figures for Q2 2025."
  },
  "analysis": {
    "query_answered": "Tesla reported total revenue of $25.5B in Q2 2025, down 4% year-over-year. Free cash flow was $1.3B.",
    "key_metrics": {
      "revenue": "$25.5B",
      "net_income": "$1.8B",
      "eps": "$0.52 diluted",
      "free_cash_flow": "$1.3B"
    },
    "notable_changes": [
      "Revenue declined 4% YoY from $26.5B in Q2 2024",
      "Net income declined 45% YoY from $3.2B"
    ],
    "data_gaps": []
  },
  "investment": {
    "positive_signals": [
      "Free cash flow of $1.3B demonstrates continued liquidity despite revenue pressure",
      "Energy generation & storage revenue grew 67% YoY to $3.0B"
    ],
    "negative_signals": [
      "Net income fell 45% YoY to $1.8B, reflecting significant automotive margin compression",
      "Automotive revenue declined 7% YoY to $19.8B"
    ],
    "management_guidance": "Management targets positive free cash flow for full year 2025 and continues to invest in next-generation vehicle platforms.",
    "investment_considerations": "Tesla demonstrates resilient cash generation relative to net income compression, supported by strong energy segment growth. However, the 45% decline in net income and 7% automotive revenue drop signal near-term demand and margin headwinds. Investors should monitor the pace of next-gen vehicle ramp and energy segment scaling as indicators of medium-term recovery.",
    "disclaimer": "This is not personalised financial advice. Consult a licensed financial advisor before making investment decisions."
  },
  "risk_assessment": {
    "risks": [
      {
        "risk_factor": "Automotive revenue decline",
        "category": "market",
        "severity": "high",
        "evidence_from_document": "Automotive revenues declined 7% YoY to $19.8B in Q2 2025, driven by lower average selling prices and reduced deliveries.",
        "recommended_mitigation": "Monitor energy and services segment growth as an offsetting revenue stream; review ASP trends across vehicle lines."
      },
      {
        "risk_factor": "Net income margin compression",
        "category": "operational",
        "severity": "high",
        "evidence_from_document": "Net income declined 45% YoY to $1.8B, with automotive gross margin falling to 14.4%.",
        "recommended_mitigation": "Track progress on cost reduction initiatives and next-generation platform manufacturing efficiency."
      }
    ]
  },
  "raw_output": "..."
}
```

---

## 10. Suggested Bonus Improvements

### 1. Async task queue with Redis + RQ
Move `run_analysis()` into an RQ worker so the API returns a `job_id`
immediately (202 Accepted) and clients poll `GET /jobs/{job_id}` for results.
This prevents HTTP timeout on large documents.
```python
from rq import Queue
from redis import Redis
q = Queue(connection=Redis())
job = q.enqueue(run_analysis, query, file_path)
```

### 2. Parallel agent execution
The `verification` and `analysis` tasks are independent. Switch to
`Process.hierarchical` with a manager LLM to run them concurrently,
cutting total response time roughly in half.

### 3. Streaming via Server-Sent Events
Use FastAPI `StreamingResponse` + crewai's `step_callback` to stream
agent reasoning steps to the client in real-time.

### 4. Document chunking for large PDFs
For filings over 100 pages, split into chunks, run analysis on each, then
use a synthesis task to merge results. Prevents context-window overflow
on long 10-K filings.

### 5. Model fallback
```python
try:
    llm = LLM(model="openai/gpt-4o", api_key=_api_key)
except RateLimitError:
    llm = LLM(model="openai/gpt-4o-mini", api_key=_api_key)
```

### 6. PostgreSQL migration
Replace SQLite with PostgreSQL for multi-worker deployments.
The `database.py` interface (create/complete/fail/get/list) is designed
as an abstraction layer â€” swap the implementation without touching any
other file.

### 7. API key authentication
```python
from fastapi.security import APIKeyHeader
api_key_header = APIKeyHeader(name="X-API-Key")
```
Add as a dependency to `/analyze` for production deployments.
